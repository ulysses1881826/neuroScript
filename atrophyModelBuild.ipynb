{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "import torch\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型想法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./csvFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = \"/home/ubuntu/Desktop/ADNI_MRI_Image/neuroScript/csvFiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "volner = pd.read_csv(os.path.join(csvPath,\"Freesurfer_S1200_Extensively_Processed_fMRI_Data.csv\"))\n",
    "volner = volner.dropna()\n",
    "MCI_freesurfer_Out = pd.read_csv(os.path.join(csvPath,\"MCI_amyloid_pos_freeStatsOut.csv\"))\n",
    "volner['Label'] = 0\n",
    "MCI_freesurfer_Out['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([volner, MCI_freesurfer_Out], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "      ..\n",
       "297    1\n",
       "298    1\n",
       "299    1\n",
       "300    1\n",
       "301    1\n",
       "Name: Label, Length: 1415, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(os.path.join(csvPath,'all_data_0307.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['FS_L_ThalamusProper_Vol', 'FS_L_Caudate_Vol', 'FS_L_Putamen_Vol', 'FS_L_Pallidum_Vol', \n",
    "            'FS_L_Hippo_Vol', 'FS_L_Amygdala_Vol', 'FS_L_AccumbensArea_Vol', 'FS_R_ThalamusProper_Vol', \n",
    "            'FS_R_Caudate_Vol', 'FS_R_Putamen_Vol', 'FS_R_Pallidum_Vol', 'FS_R_Hippo_Vol', 'FS_R_Amygdala_Vol', \n",
    "            'FS_R_AccumbensArea_Vol', 'FS_L_Bankssts_Thck', 'FS_L_Caudalanteriorcingulate_Thck', \n",
    "            'FS_L_Caudalmiddlefrontal_Thck', 'FS_L_Cuneus_Thck', 'FS_L_Entorhinal_Thck', 'FS_L_Fusiform_Thck', \n",
    "            'FS_L_Inferiorparietal_Thck', 'FS_L_Inferiortemporal_Thck', 'FS_L_Isthmuscingulate_Thck', \n",
    "            'FS_L_Lateraloccipital_Thck', 'FS_L_Lateralorbitofrontal_Thck', 'FS_L_Lingual_Thck', \n",
    "            'FS_L_Medialorbitofrontal_Thck', 'FS_L_Middletemporal_Thck', 'FS_L_Parahippocampal_Thck', \n",
    "            'FS_L_Paracentral_Thck', 'FS_L_Parsopercularis_Thck', 'FS_L_Parsorbitalis_Thck', \n",
    "            'FS_L_Parstriangularis_Thck', 'FS_L_Pericalcarine_Thck', 'FS_L_Postcentral_Thck', \n",
    "            'FS_L_Posteriorcingulate_Thck', 'FS_L_Precentral_Thck', 'FS_L_Precuneus_Thck', \n",
    "            'FS_L_Rostralanteriorcingulate_Thck', 'FS_L_Rostralmiddlefrontal_Thck', 'FS_L_Superiorfrontal_Thck', \n",
    "            'FS_L_Superiorparietal_Thck', 'FS_L_Superiortemporal_Thck', 'FS_L_Supramarginal_Thck', \n",
    "            'FS_L_Frontalpole_Thck', 'FS_L_Temporalpole_Thck', 'FS_L_Transversetemporal_Thck', \n",
    "            'FS_L_Insula_Thck', 'FS_R_Bankssts_Thck', 'FS_R_Caudalanteriorcingulate_Thck', \n",
    "            'FS_R_Caudalmiddlefrontal_Thck', 'FS_R_Cuneus_Thck', 'FS_R_Entorhinal_Thck', 'FS_R_Fusiform_Thck', \n",
    "            'FS_R_Inferiorparietal_Thck', 'FS_R_Inferiortemporal_Thck', 'FS_R_Isthmuscingulate_Thck', \n",
    "            'FS_R_Lateraloccipital_Thck', 'FS_R_Lateralorbitofrontal_Thck', 'FS_R_Lingual_Thck', \n",
    "            'FS_R_Medialorbitofrontal_Thck', 'FS_R_Middletemporal_Thck', 'FS_R_Parahippocampal_Thck', \n",
    "            'FS_R_Paracentral_Thck', 'FS_R_Parsopercularis_Thck', 'FS_R_Parsorbitalis_Thck', \n",
    "            'FS_R_Parstriangularis_Thck', 'FS_R_Pericalcarine_Thck', 'FS_R_Postcentral_Thck', \n",
    "            'FS_R_Posteriorcingulate_Thck', 'FS_R_Precentral_Thck', 'FS_R_Precuneus_Thck', \n",
    "            'FS_R_Rostralanteriorcingulate_Thck', 'FS_R_Rostralmiddlefrontal_Thck', \n",
    "            'FS_R_Superiorfrontal_Thck', 'FS_R_Superiorparietal_Thck', 'FS_R_Superiortemporal_Thck',\n",
    "            'FS_R_Supramarginal_Thck', 'FS_R_Frontalpole_Thck', 'FS_R_Temporalpole_Thck', \n",
    "            'FS_R_Transversetemporal_Thck', 'FS_R_Insula_Thck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert pd.Dataframe to feature(np.array) here ## with label here\n",
    "def Convert2FeatureVectorWithLabel(pdFrame):\n",
    "    subject = []\n",
    "    pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf'] = 1.0 / pdFrame['FS_BrainSeg_Vol_No_Vent_Surf']\n",
    "    X = []\n",
    "    x =[]\n",
    "    weighted_mean_feature_dict = {}\n",
    "    for feature in features:\n",
    "        if feature.split('_')[-1] == 'Thck':\n",
    "            feature_area = feature[0:-4] + 'Area'\n",
    "            weighted_mean_feature_dict[feature] = (pdFrame[feature] * pdFrame[feature_area]).sum() / pdFrame[feature_area].sum()\n",
    "        if feature.split('_')[-1] == 'Vol':\n",
    "            weighted_mean_feature_dict[feature] = (pdFrame[feature] * pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf']).sum() / pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf'].sum()   \n",
    "    variance_feature_dict = {}\n",
    "    for feature in features:\n",
    "        if feature.split('_')[-1] == 'Thck':\n",
    "            feature_area = feature[0:-4] + 'Area'\n",
    "            variance_feature_dict[feature] = np.sqrt((pdFrame[feature_area] * np.square(pdFrame[feature] - weighted_mean_feature_dict[feature])).sum() / (pdFrame[feature_area].sum() - (np.square(pdFrame[feature_area])).sum() / pdFrame[feature_area].sum()))\n",
    "        if feature.split('_')[-1] == 'Vol':\n",
    "            variance_feature_dict[feature] = np.sqrt((pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"] * np.square(pdFrame[feature] - weighted_mean_feature_dict[feature])).sum() / (pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum() - (np.square(pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"])).sum() / pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum()))\n",
    "    for index,row in pdFrame.iterrows():\n",
    "        for feature in features:\n",
    "            x.append((np.float(row[feature]) - weighted_mean_feature_dict[feature]) / variance_feature_dict[feature])\n",
    "#         x.append(row['Subject'])\n",
    "        subject.append(row['Subject'])\n",
    "        x.append(row['Label'])\n",
    "        X.append(x)\n",
    "        x =[]\n",
    "        if len(X) % 100 == 0:\n",
    "            print(\"finish 100\")\n",
    "    X = np.array(X)\n",
    "    return X, subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert pd.Dataframe to feature(np.array) here ## No label here\n",
    "def Convert2FeatureVector(pdFrame):\n",
    "    subject = []\n",
    "    pdFrame = pdFrame.dropna()\n",
    "    pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf'] = 1.0 / pdFrame['FS_BrainSeg_Vol_No_Vent_Surf']\n",
    "    X = []\n",
    "    x =[]\n",
    "    weighted_mean_feature_dict = {}\n",
    "    for feature in features:\n",
    "        if feature.split('_')[-1] == 'Thck':\n",
    "            feature_area = feature[0:-4] + 'Area'\n",
    "            weighted_mean_feature_dict[feature] = (pdFrame[feature] * pdFrame[feature_area]).sum() / pdFrame[feature_area].sum()\n",
    "        if feature.split('_')[-1] == 'Vol':\n",
    "            weighted_mean_feature_dict[feature] = (pdFrame[feature] * pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf']).sum() / pdFrame['reverse_FS_BrainSeg_Vol_No_Vent_Surf'].sum()   \n",
    "    variance_feature_dict = {}\n",
    "    for feature in features:\n",
    "        if feature.split('_')[-1] == 'Thck':\n",
    "            feature_area = feature[0:-4] + 'Area'\n",
    "            variance_feature_dict[feature] = np.sqrt((pdFrame[feature_area] * np.square(pdFrame[feature] - weighted_mean_feature_dict[feature])).sum() / (pdFrame[feature_area].sum() - (np.square(pdFrame[feature_area])).sum() / pdFrame[feature_area].sum()))\n",
    "        if feature.split('_')[-1] == 'Vol':\n",
    "            variance_feature_dict[feature] = np.sqrt((pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"] * np.square(pdFrame[feature] - weighted_mean_feature_dict[feature])).sum() / (pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum() - (np.square(pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"])).sum() / pdFrame[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum()))\n",
    "    for index,row in pdFrame.iterrows():\n",
    "        for feature in features:\n",
    "            x.append((np.float(row[feature]) - weighted_mean_feature_dict[feature]) / variance_feature_dict[feature])\n",
    "#         x.append(row['Subject'])\n",
    "        X.append(x)\n",
    "        subject.append(row['Subject'])\n",
    "        x =[]\n",
    "        if len(X) % 100 == 0:\n",
    "            print(\"finish 100\")\n",
    "    X = np.array(X)\n",
    "    return X, subject\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "(1415, 83)\n",
      "(1415, 82) (1415,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9952941176470588"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector_with_Label, Vector_subject = Convert2FeatureVectorWithLabel(all_data)\n",
    "print(Vector_with_Label.shape)\n",
    "\n",
    "print(Vector_with_Label[:,0:-1].shape,Vector_with_Label[:,-1].shape)\n",
    "X = Vector_with_Label[:,0:-1]\n",
    "y = Vector_with_Label[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(64,64), random_state=1,max_iter=700)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "finish 100\n",
      "(302,) (1113,)\n",
      "(1415,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7435294117647059"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCI_vector,MCI_subject = Convert2FeatureVector(MCI_freesurfer_Out)\n",
    "MCI_vector.shape\n",
    "\n",
    "volner_vector, volner_subject = Convert2FeatureVector(volner)\n",
    "volner_vector.shape\n",
    "\n",
    "MCI_label = np.ones(302)\n",
    "volner_label = np.zeros(1113)\n",
    "print(MCI_label.shape,volner_label.shape)\n",
    "y = np.concatenate((MCI_label,volner_label))\n",
    "print(y.shape)\n",
    "\n",
    "X = np.concatenate((MCI_vector,volner_vector))\n",
    "X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(64,128), random_state=1,max_iter=700)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "990/990 [==============================] - 0s 424us/step - loss: 0.4292 - accuracy: 0.8394\n",
      "Epoch 2/10\n",
      "990/990 [==============================] - 0s 204us/step - loss: 0.1593 - accuracy: 0.9798\n",
      "Epoch 3/10\n",
      "990/990 [==============================] - 0s 194us/step - loss: 0.0716 - accuracy: 0.9919\n",
      "Epoch 4/10\n",
      "990/990 [==============================] - 0s 187us/step - loss: 0.0376 - accuracy: 0.9980\n",
      "Epoch 5/10\n",
      "990/990 [==============================] - 0s 183us/step - loss: 0.0226 - accuracy: 0.9980\n",
      "Epoch 6/10\n",
      "990/990 [==============================] - 0s 170us/step - loss: 0.0148 - accuracy: 0.9990\n",
      "Epoch 7/10\n",
      "990/990 [==============================] - 0s 174us/step - loss: 0.0106 - accuracy: 0.9990\n",
      "Epoch 8/10\n",
      "990/990 [==============================] - 0s 194us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "990/990 [==============================] - 0s 175us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "990/990 [==============================] - 0s 200us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "425/425 [==============================] - 0s 110us/step\n",
      "Accuracy: 99.76\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=82, activation='relu'))\n",
    "model.add(Dense(8, activation='relu',name = 'my_layer'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(MCI_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(csvPath,\"all_data.tsv\"),Vector_with_Label[:,0:-1],delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(csvPath,\"MCI_sne.tsv\"),MCI_vector,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the start of draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt(os.path.join(csvPath,'pima-indians-diabetes.data.csv'), delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt(os.path.join(csvPath,'pima-indians-diabetes.data.csv'), delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=15, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(64,100), random_state=1,max_iter=700).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volner_no_Nan = volner.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weighted_mean_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean_feature_dict = {}\n",
    "for feature in features:\n",
    "    if feature.split('_')[-1] == 'Thck':\n",
    "        feature_area = feature[0:-4] + 'Area'\n",
    "        weighted_mean_feature_dict[feature] = (volner_no_Nan[feature] * volner_no_Nan[feature_area]).sum() / volner_no_Nan[feature_area].sum()\n",
    "    if feature.split('_')[-1] == 'Vol':\n",
    "        weighted_mean_feature_dict[feature] = (volner_no_Nan[feature] * volner_no_Nan['reverse_FS_BrainSeg_Vol_No_Vent_Surf']).sum() / volner_no_Nan['reverse_FS_BrainSeg_Vol_No_Vent_Surf'].sum()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_feature_dict = {}\n",
    "for feature in features:\n",
    "    if feature.split('_')[-1] == 'Thck':\n",
    "        feature_area = feature[0:-4] + 'Area'\n",
    "        variance_feature_dict[feature] = np.sqrt((volner_no_Nan[feature_area] * np.square(volner_no_Nan[feature] - weighted_mean_feature_dict[feature])).sum() / (volner_no_Nan[feature_area].sum() - (np.square(volner_no_Nan[feature_area])).sum() / volner_no_Nan[feature_area].sum()))\n",
    "    if feature.split('_')[-1] == 'Vol':\n",
    "        variance_feature_dict[feature] = np.sqrt((volner_no_Nan[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"] * np.square(volner_no_Nan[feature] - weighted_mean_feature_dict[feature])).sum() / (volner_no_Nan[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum() - (np.square(volner_no_Nan[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"])).sum() / volner_no_Nan[\"reverse_FS_BrainSeg_Vol_No_Vent_Surf\"].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volner_no_Nan['reverse_FS_BrainSeg_Vol_No_Vent_Surf'] = 1.0 / volner_no_Nan['FS_BrainSeg_Vol_No_Vent_Surf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variance_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "x =[]\n",
    "for index,row in volner_no_Nan.iterrows():\n",
    "    for feature in features:\n",
    "        x.append((np.float(row[feature]) - weighted_mean_feature_dict[feature]) / variance_feature_dict[feature])\n",
    "    X.append(x)\n",
    "    x =[]\n",
    "    if len(X) % 100 == 0:\n",
    "        print(\"finish 100\")\n",
    "X = np.array(X)      \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectFeatures2 = [\"left-wthi-bankssts\",\"left-wthi-caudalanteriorcingulate\",\"left-wthi-caudalmiddlefrontal\",\n",
    "                  \"left-wthi-cuneus\",\"left-wthi-entorhinal\",\"left-wthi-fusiform\",\"left-wthi-inferiorparietal\",\n",
    "                  \"left-wthi-inferiortemporal\",\"left-wthi-isthmuscingulate\",\"left-wthi-lateraloccipital\",\n",
    "                  \"left-wthi-lateralorbitofrontal\",\"left-wthi-lingual\",\"left-wthi-medialorbitofrontal\",\n",
    "                  \"left-wthi-middletemporal\",\"left-wthi-parahippocampal\",\"left-wthi-paracentral\",\n",
    "                  \"left-wthi-parsopercularis\",\"left-wthi-parsorbitalis\",\"left-wthi-parstriangularis\",\n",
    "                  \"left-wthi-pericalcarine\",\"left-wthi-postcentral\",\"left-wthi-posteriorcingulate\",\n",
    "                  \"left-wthi-precentral\",\"left-wthi-precuneus\",\"left-wthi-rostralanteriorcingulate\",\n",
    "                  \"left-wthi-rostralmiddlefrontal\",\"left-wthi-superiorfrontal\",\"left-wthi-superiorparietal\",\n",
    "                  \"left-wthi-superiortemporal\",\"left-wthi-supramarginal\",\"left-wthi-frontalpole\",\n",
    "                  \"left-wthi-temporalpole\",\"left-wthi-transversetemporal\",\"left-wthi-insula\",\"right-wthi-bankssts\",\n",
    "                  \"right-wthi-caudalanteriorcingulate\",\"right-wthi-caudalmiddlefrontal\",\"right-wthi-cuneus\",\n",
    "                  \"right-wthi-entorhinal\",\"right-wthi-fusiform\",\"right-wthi-inferiorparietal\",\n",
    "                  \"right-wthi-inferiortemporal\",\"right-wthi-isthmuscingulate\",\"right-wthi-lateraloccipital\",\n",
    "                  \"right-wthi-lateralorbitofrontal\",\"right-wthi-lingual\",\"right-wthi-medialorbitofrontal\",\n",
    "                  \"right-wthi-middletemporal\",\"right-wthi-parahippocampal\",\"right-wthi-paracentral\",\n",
    "                  \"right-wthi-parsopercularis\",\"right-wthi-parsorbitalis\",\"right-wthi-parstriangularis\",\n",
    "                  \"right-wthi-pericalcarine\",\"right-wthi-postcentral\",\"right-wthi-posteriorcingulate\",\n",
    "                  \"right-wthi-precentral\",\"right-wthi-precuneus\",\"right-wthi-rostralanteriorcingulate\",\n",
    "                  \"right-wthi-rostralmiddlefrontal\",\"right-wthi-superiorfrontal\",\"right-wthi-superiorparietal\",\n",
    "                  \"right-wthi-superiortemporal\",\"right-wthi-supramarginal\",\"right-wthi-frontalpole\",\n",
    "                  \"right-wthi-temporalpole\",\"right-wthi-transversetemporal\",\"right-wthi-insula\",\n",
    "                  \"right-svol-Thalamus\",\"right-svol-Caudate\",\"right-svol-Putamen\",\"right-svol-Pallidum\",\n",
    "                  \"right-svol-Hippocampus\",\"right-svol-Amygdala\",\"right-svol-Accumbens\",\"left-svol-Thalamus\",\n",
    "                  \"left-svol-Caudate\",\"left-svol-Putamen\",\"left-svol-Pallidum\",\"left-svol-Hippocampus\",\n",
    "                  \"left-svol-Amygdala\",\"left-svol-Accumbens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the jwt token (run bl login to create this file)\n",
    "jwt_file = open(os.environ['HOME']+'/.config/brainlife.io/.jwt', mode='r')\n",
    "jwt = jwt_file.read()\n",
    "#query datasets records\n",
    "find = { \n",
    "    '_group_id': '69', #see project detail page \n",
    "    'service': 'brainlife/app-freesurfer', \n",
    "    #'service_branch': '0.0.5',\n",
    "    'status': 'finished' }\n",
    "params = { \n",
    "    'limit': 100, \n",
    "    'select': 'config._inputs.meta', #we just want product and meta\n",
    "    'find': json.JSONEncoder().encode(find) }\n",
    "res = requests.get('https://brainlife.io/api/amaretti/task', params=params, headers={'Authorization': 'Bearer '+jwt})\n",
    "if res.status_code != 200:\n",
    "    raise Exception(\"failed to download datasets list:\"+res.status_code)\n",
    "tasks = res.json()[\"tasks\"]\n",
    "for task in tasks:\n",
    "    taskid=task[\"_id\"]\n",
    "    subject=task[\"config\"][\"_inputs\"][0][\"meta\"][\"subject\"]\n",
    "    print(taskid, subject)\n",
    "    url = 'https://brainlife.io/api/amaretti/task/download/'+taskid+'/freesurfer/output/stats?at='+jwt\n",
    "    res = requests.get(url, allow_redirects=True)\n",
    "    open(subject+'.tar.gz', 'wb').write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://brainlife.io/api/amaretti/resource',headers={'Authorization': 'Bearer '+jwt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This verison is for all volume feature\n",
    "selectFeatures1 = [\"left-vol-bankssts\",\"left-vol-caudalanteriorcingulate\",\"left-vol-caudalmiddlefrontal\",\n",
    "                  \"left-vol-cuneus\",\"left-vol-entorhinal\",\"left-vol-fusiform\",\"left-vol-inferiorparietal\",\n",
    "                  \"left-vol-inferiortemporal\",\"left-vol-isthmuscingulate\",\"left-vol-lateraloccipital\",\n",
    "                  \"left-vol-lateralorbitofrontal\",\"left-vol-lingual\",\"left-vol-medialorbitofrontal\",\n",
    "                  \"left-vol-middletemporal\",\"left-vol-parahippocampal\",\"left-vol-paracentral\",\n",
    "                  \"left-vol-parsopercularis\",\"left-vol-parsorbitalis\",\"left-vol-parstriangularis\",\n",
    "                  \"left-vol-pericalcarine\",\"left-vol-postcentral\",\"left-vol-posteriorcingulate\",\n",
    "                  \"left-vol-precentral\",\"left-vol-precuneus\",\"left-vol-rostralanteriorcingulate\",\n",
    "                  \"left-vol-rostralmiddlefrontal\",\"left-vol-superiorfrontal\",\"left-vol-superiorparietal\",\n",
    "                  \"left-vol-superiortemporal\",\"left-vol-supramarginal\",\"left-vol-frontalpole\",\n",
    "                  \"left-vol-temporalpole\",\"left-vol-transversetemporal\",\"left-vol-insula\",\"right-vol-bankssts\",\n",
    "                  \"right-vol-caudalanteriorcingulate\",\"right-vol-caudalmiddlefrontal\",\"right-vol-cuneus\",\n",
    "                  \"right-vol-entorhinal\",\"right-vol-fusiform\",\"right-vol-inferiorparietal\",\n",
    "                  \"right-vol-inferiortemporal\",\"right-vol-isthmuscingulate\",\"right-vol-lateraloccipital\",\n",
    "                  \"right-vol-lateralorbitofrontal\",\"right-vol-lingual\",\"right-vol-medialorbitofrontal\",\n",
    "                  \"right-vol-middletemporal\",\"right-vol-parahippocampal\",\"right-vol-paracentral\",\n",
    "                  \"right-vol-parsopercularis\",\"right-vol-parsorbitalis\",\"right-vol-parstriangularis\",\n",
    "                  \"right-vol-pericalcarine\",\"right-vol-postcentral\",\"right-vol-posteriorcingulate\",\n",
    "                  \"right-vol-precentral\",\"right-vol-precuneus\",\"right-vol-rostralanteriorcingulate\",\n",
    "                  \"right-vol-rostralmiddlefrontal\",\"right-vol-superiorfrontal\",\"right-vol-superiorparietal\",\n",
    "                  \"right-vol-superiortemporal\",\"right-vol-supramarginal\",\"right-vol-frontalpole\",\n",
    "                  \"right-vol-temporalpole\",\"right-vol-transversetemporal\",\"right-vol-insula\",\n",
    "                  \"right-svol-Thalamus\",\"right-svol-Caudate\",\"right-svol-Putamen\",\"right-svol-Pallidum\",\n",
    "                  \"right-svol-Hippocampus\",\"right-svol-Amygdala\",\"right-svol-Accumbens\",\"left-svol-Thalamus\",\n",
    "                  \"left-svol-Caudate\",\"left-svol-Putamen\",\"left-svol-Pallidum\",\"left-svol-Hippocampus\",\n",
    "                  \"left-svol-Amygdala\",\"left-svol-Accumbens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(type(x))\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volner_tensor = torch.tensor(volner_vector)\n",
    "volner_tensor2 = torch.from_numpy(volner_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
